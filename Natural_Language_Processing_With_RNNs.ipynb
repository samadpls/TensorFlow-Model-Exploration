{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTAXArCNqihB4HWX3WFGix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samadpls/TensorFlow-Model-Exploration/blob/colab/Natural_Language_Processing_With_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dh1A5c8hYRcr"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc5d8FM1t74H",
        "outputId": "f11b34c4-23a2-406a-d082-0b9968285a2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_to_file,'rb') as file:\n",
        "  text = file.read().decode(encoding='utf-8')\n",
        "print(len(text))\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgTeakHhuSI6",
        "outputId": "c9b84d78-b7df-4759-8cf6-fecc319de21e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab =  sorted(set(text))\n",
        "char2idx =  {u:i for i,u in enumerate(vocab)}\n",
        "idx2char =  np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int =  text_to_int(text)"
      ],
      "metadata": {
        "id": "ITguohUru6CB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Text-->\",text[:14])\n",
        "print(\"Encoded--> \", text_to_int(text[:14]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yevVGVJwvtI0",
        "outputId": "cc6e0a8f-4ed4-4033-f3ea-200450cc40db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text--> First Citizen:\n",
            "Encoded-->  [18 47 56 57 58  1 15 47 58 47 64 43 52 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return \"\".join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_to_int(text[:14])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMhq8TVxv8EU",
        "outputId": "34e3b3e0-77f7-489b-85e7-5dd3bd0e41de"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch =  len(text)//(seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "SRv6AmLowaOa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences =  char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "Am-XvSqXxLFP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:1]\n",
        "  target_text =  chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "25s2cLwcxb6Y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in dataset.take(2):\n",
        "  print(\"Example\")\n",
        "  print(\"input\")\n",
        "  print(int_to_text(x))\n",
        "  print('output')\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hya_vVukx0SD",
        "outputId": "a808d892-9666-43ab-fc4a-d28f2721ce35"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example\n",
            "input\n",
            "F\n",
            "output\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "Example\n",
            "input\n",
            "a\n",
            "output\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "vocab_size = len(vocab)\n",
        "embeding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "buffer_size = 10000\n",
        "\n",
        "data = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder= True)"
      ],
      "metadata": {
        "id": "gc1R_ygsyPPH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embeding_dim, rnn_units, batch_size):\n",
        "  model =  tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size,embeding_dim,\n",
        "      batch_input_shape= [batch_size,None]),\n",
        "      tf.keras.layers.LSTM(rnn_units,\n",
        "                           return_sequences=True, # want on every intermediate part\n",
        "                           stateful=True,\n",
        "                           recurrent_initializer='glorot_uniform'),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "      ])\n",
        "  return model\n",
        "model =  build_model(vocab_size,embeding_dim,rnn_units,batch_size)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J2g3wUry3vx",
        "outputId": "3d1538ff-d9e5-428a-853b-eb75458a9ea1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in data.take(1):\n",
        "  example_prd= model(input_example)\n",
        "  print(example_prd.shape,\"batch size, sequence length, vocab_size\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxOSfuS83X2k",
        "outputId": "5a0b45d7-6617-448f-81ac-c80ad70ba1d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 1, 65) batch size, sequence length, vocab_size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(example_prd))\n",
        "print(example_prd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yeuz-fmT_fjR",
        "outputId": "916b043a-9fae-451a-b032-d9b829095602"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 5.0332240e-04  2.8083497e-03 -1.2157870e-03 ...  2.0423471e-03\n",
            "    9.4294082e-05  1.7096852e-03]]\n",
            "\n",
            " [[ 1.2716302e-03 -5.1937285e-03  2.2551650e-03 ...  1.7016797e-03\n",
            "    5.7998821e-03 -8.0865882e-03]]\n",
            "\n",
            " [[-1.6213255e-03 -3.9232830e-03  2.0987634e-03 ... -3.2209423e-03\n",
            "   -4.5928489e-03  8.2668883e-04]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 3.8223004e-03 -2.9790751e-04 -3.7611085e-03 ... -1.2006077e-03\n",
            "    8.3139213e-04 -1.8752748e-03]]\n",
            "\n",
            " [[-1.2047737e-03  2.0690700e-03 -3.3193519e-03 ...  2.8579407e-03\n",
            "   -4.0857475e-03 -8.7283412e-03]]\n",
            "\n",
            " [[ 5.0332240e-04  2.8083497e-03 -1.2157870e-03 ...  2.0423471e-03\n",
            "    9.4294082e-05  1.7096852e-03]]], shape=(64, 1, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels,logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)"
      ],
      "metadata": {
        "id": "Q2E3a1XBAPQi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "Wb1FC7vjBGAy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint =  './training_checkpoints'\n",
        "chckpoint_prefix = os.path.join(checkpoint,'ckpt_{epoch}')\n",
        "\n",
        "chckpoint_callback= tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=chckpoint_prefix,\n",
        "    save_weights_only=True\n",
        "    )"
      ],
      "metadata": {
        "id": "I-5YQdWdBX-k"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=  model.fit(data,epochs=0,callbacks=[chckpoint_callback])"
      ],
      "metadata": {
        "id": "MRUWNIpvB-Z3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XQULpybHCIfP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}